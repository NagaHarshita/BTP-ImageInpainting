{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nimport argparse\nimport numpy as np\nfrom PIL import  Image\nimport pickle as pkl\n\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision.utils import make_grid, save_image\nfrom torch.utils.data import Dataset, DataLoader\nfrom tensorboardX import SummaryWriter\nfrom tqdm import tqdm\nfrom copy import deepcopy\nimport cv2\nimport matplotlib.pyplot as plt\nimport os\n\n!mkdir checkpoint\n!mkdir generated_imgs\n!mkdir output_imgs\n!mkdir real_imgs\n!mkdir fake_imgs\n!mkdir real_test_imgs\n!pip install tensorboardX\n!pip install pytorch-fid","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:40:13.15472Z","iopub.execute_input":"2021-11-17T09:40:13.155147Z","iopub.status.idle":"2021-11-17T09:40:37.744903Z","shell.execute_reply.started":"2021-11-17T09:40:13.155058Z","shell.execute_reply":"2021-11-17T09:40:37.74388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Storing the path to dataset in the lists**","metadata":{}},{"cell_type":"code","source":"TrainA = []\nTrainB = []\nTestA = []\nTestB = []\n\nfor dirname, _, filenames in os.walk('/kaggle/input/custom/celebA_dataset/Training/TrainA/'):\n    for filename in filenames:\n        TrainA.append(os.path.join(dirname, filename))\n        TrainB.append(os.path.join('/kaggle/input/custom/celebA_dataset/Training/TrainB/', filename))\n        \nfor dirname, _, filenames in os.walk('/kaggle/input/custom/celebA_dataset/Test/TestB/'):\n    for filename in filenames:\n        TestA.append(os.path.join(dirname, filename))\n        TestB.append(os.path.join('/kaggle/input/custom/celebA_dataset/Test/TestB/', filename))","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:40:37.748747Z","iopub.execute_input":"2021-11-17T09:40:37.749007Z","iopub.status.idle":"2021-11-17T09:40:44.946514Z","shell.execute_reply.started":"2021-11-17T09:40:37.748974Z","shell.execute_reply":"2021-11-17T09:40:44.945725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preprocess the dataset with a random rectangular mask**","metadata":{}},{"cell_type":"code","source":"class InpaintDataset(Dataset):\n    def __init__(self, real_image_paths, mask_image_paths, transform=False):\n        self.real_image_paths = real_image_paths\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.real_image_paths)\n\n    def __getitem__(self, idx):\n        real_image_filepath = self.real_image_paths[idx]\n        real_img = Image.open(real_image_filepath).convert(\"RGB\")\n        mask_img = InpaintDataset.read_val_mask(self, real_image_filepath)\n       \n    \n        if self.transform is not None:\n            real_img = self.transform(real_img)\n            mask_img = self.transform(mask_img)\n        \n        return real_img, mask_img\n    \n    def read_val_mask(self, path):\n        \"\"\"\n        Read masks from val mask data\n        \"\"\"\n        image = cv2.imread(path)[...,::-1]\n        height, width, channels = image.shape\n        pos =  random.randint(50, 120)\n        a = random.randint(0, height-pos)\n        b = random.randint(0, width-pos)\n        c = a + pos\n        d = b + pos\n        start_point = (a, b)\n        end_point = (c, d)\n        color = (255, 255, 255)\n        thickness = -1\n        image = Image.fromarray(image.astype(np.uint8))\n        image = np.array(image)\n        image = cv2.rectangle(image, start_point, end_point, color, thickness)\n        return transforms.ToPILImage()(image)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:40:45.154992Z","iopub.execute_input":"2021-11-17T09:40:45.155417Z","iopub.status.idle":"2021-11-17T09:40:45.168389Z","shell.execute_reply.started":"2021-11-17T09:40:45.155362Z","shell.execute_reply":"2021-11-17T09:40:45.167208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LinearLrDecay(object):\n    def __init__(self, optimizer, start_lr, end_lr, decay_start_step, decay_end_step):\n\n        assert start_lr > end_lr\n        self.optimizer = optimizer\n        self.delta = (start_lr - end_lr) / (decay_end_step - decay_start_step)\n        self.decay_start_step = decay_start_step\n        self.decay_end_step = decay_end_step\n        self.start_lr = start_lr\n        self.end_lr = end_lr\n\n    def step(self, current_step):\n        if current_step <= self.decay_start_step:\n            lr = self.start_lr\n        elif current_step >= self.decay_end_step:\n            lr = self.end_lr\n        else:\n            lr = self.start_lr - self.delta * (current_step - self.decay_start_step)\n            for param_group in self.optimizer.param_groups:\n                param_group['lr'] = lr\n        return lr\n\ndef inits_weight(m):\n        if type(m) == nn.Linear:\n                nn.init.xavier_uniform(m.weight.data, 1.)\n\ndef save_checkpoint(states,is_best, output_dir, epoch,\n                    filename='checkpoint.pth'):\n    torch.save(states, os.path.join(output_dir, filename+str(epoch)))\n    if is_best:\n        torch.save(states, os.path.join(output_dir, 'checkpoint_best.pth'))","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:40:45.184623Z","iopub.execute_input":"2021-11-17T09:40:45.185226Z","iopub.status.idle":"2021-11-17T09:40:45.201284Z","shell.execute_reply.started":"2021-11-17T09:40:45.185181Z","shell.execute_reply":"2021-11-17T09:40:45.200365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def DiffAugment(x, policy='', channels_first=True):\n    if policy:\n        if not channels_first:\n            x = x.permute(0, 3, 1, 2)\n        for p in policy.split(','):\n            for f in AUGMENT_FNS[p]:\n                x = f(x)\n        if not channels_first:\n            x = x.permute(0, 2, 3, 1)\n        x = x.contiguous()\n    return x\n\n\ndef rand_brightness(x):\n    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n    return x\n\n\ndef rand_saturation(x):\n    x_mean = x.mean(dim=1, keepdim=True)\n    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n    return x\n\n\ndef rand_contrast(x):\n    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n    return x\n\n\ndef rand_translation(x, ratio=0.2):\n    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n    grid_batch, grid_x, grid_y = torch.meshgrid(\n        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n    )\n    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n    return x\n\n\ndef rand_cutout(x, ratio=0.5):\n    if random.random() < 0.3:\n        cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n        offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n        offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n        grid_batch, grid_x, grid_y = torch.meshgrid(\n            torch.arange(x.size(0), dtype=torch.long, device=x.device),\n            torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n            torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n        )\n        grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n        grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n        mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n        mask[grid_batch, grid_x, grid_y] = 0\n        x = x * mask.unsqueeze(1)\n    return x\n\ndef rand_rotate(x, ratio=0.5):\n    k = random.randint(1,3)\n    if random.random() < ratio:\n        x = torch.rot90(x, k, [2,3])\n    return x\n\nAUGMENT_FNS = {\n    'color': [rand_brightness, rand_saturation, rand_contrast],\n    'translation': [rand_translation],\n    'cutout': [rand_cutout],\n    'rotate': [rand_rotate],\n}","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:40:45.204847Z","iopub.execute_input":"2021-11-17T09:40:45.205088Z","iopub.status.idle":"2021-11-17T09:40:45.232485Z","shell.execute_reply.started":"2021-11-17T09:40:45.205059Z","shell.execute_reply":"2021-11-17T09:40:45.231691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training hyperparameters given by code author\n\nlr_gen = 0.0001 #Learning rate for generator\nlr_dis = 0.0001 #Learning rate for discriminator\nlatent_dim = 1024 #Latent dimension\ngener_batch_size = 32 #Batch size for generator\ndis_batch_size = 32 #Batch size for discriminator\nepoch = 10 #Number of epoch\nweight_decay = 1e-3 #Weight decay\ndrop_rate = 0.5 #dropout\nn_critic = 5 #\nmax_iter = 500000\nimg_name = \"img_name\"\nlr_decay = True\n\n# architecture details by authors\nimage_size = 64 #H,W size of image for discriminator\ninitial_size = 8 #Initial size for generator\npatch_size = 4 #Patch size for generated image\nnum_classes = 1 #Number of classes for discriminator \noutput_dir = 'checkpoint' #saved model path\ndim = 768 #Embedding dimension \noptimizer = 'Adam' #Optimizer\nloss = \"wgangp_eps\" #Loss function\nphi = 1 #\nbeta1 = 0 #\nbeta2 = 0.99 #\ndiff_aug = \"translation,cutout,color\" #data augmentation\n","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:40:45.234027Z","iopub.execute_input":"2021-11-17T09:40:45.234571Z","iopub.status.idle":"2021-11-17T09:40:45.24577Z","shell.execute_reply.started":"2021-11-17T09:40:45.234528Z","shell.execute_reply":"2021-11-17T09:40:45.244903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\nclass MLP(nn.Module):\n    def __init__(self, in_feat, hid_feat=None, out_feat=None,\n                 dropout=0.):\n        super().__init__()\n        if not hid_feat:\n            hid_feat = in_feat\n        if not out_feat:\n            out_feat = in_feat\n        self.fc1 = nn.Linear(in_feat, hid_feat)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(hid_feat, out_feat)\n        self.droprateout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.fc2(x)\n        return self.droprateout(x)\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads=4, attention_dropout=0., proj_dropout=0.):\n        super().__init__()\n        self.heads = heads\n        self.scale = 1./dim**0.5\n\n        self.qkv = nn.Linear(dim, dim*3, bias=False)\n        self.attention_dropout = nn.Dropout(attention_dropout)\n        self.out = nn.Sequential(\n            nn.Linear(dim, dim),\n            nn.Dropout(proj_dropout)\n        )\n\n    def forward(self, x):\n        b, n, c = x.shape\n        qkv = self.qkv(x).reshape(b, n, 3, self.heads, c//self.heads)\n        q, k, v = qkv.permute(2, 0, 3, 1, 4)\n\n        dot = (q @ k.transpose(-2, -1)) * self.scale\n        attn = dot.softmax(dim=-1)\n        attn = self.attention_dropout(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(b, n, c)\n        x = self.out(x)\n        return x\n\nclass ImgPatches(nn.Module):\n    def __init__(self, input_channel=3, dim=768, patch_size=4):\n        super().__init__()\n        self.patch_embed = nn.Conv2d(input_channel, dim,\n                                     kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, img):\n        patches = self.patch_embed(img).flatten(2).transpose(1, 2)\n        return patches\n\ndef UpSampling(x, H, W):\n        B, N, C = x.size()\n        # print(x.size())\n        # assert N == H*W\n        x = x.permute(0, 2, 1)\n        x = x.view(-1, C, H, W)\n        x = nn.PixelShuffle(2)(x)\n        B, C, H, W = x.size()\n        x = x.view(-1, C, H*W)\n        x = x.permute(0,2,1)\n        return x, H, W\n\nclass Encoder_Block(nn.Module):\n    def __init__(self, dim, heads, mlp_ratio=4, drop_rate=0.):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.attn = Attention(dim, heads, drop_rate, drop_rate)\n        self.ln2 = nn.LayerNorm(dim)\n        self.mlp = MLP(dim, dim*mlp_ratio, dropout=drop_rate)\n\n    def forward(self, x):\n        x1 = self.ln1(x)\n        x = x + self.attn(x1)\n        x2 = self.ln2(x)\n        x = x + self.mlp(x2)\n        return x\n\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, depth, dim, heads, mlp_ratio=4, drop_rate=0.):\n        super().__init__()\n        self.Encoder_Blocks = nn.ModuleList([\n            Encoder_Block(dim, heads, mlp_ratio, drop_rate)\n            for i in range(depth)])\n\n    def forward(self, x):\n        for Encoder_Block in self.Encoder_Blocks:\n            x = Encoder_Block(x)\n        return x\n\nclass Generator(nn.Module):\n    \"\"\"docstring for Generator\"\"\"\n    def __init__(self, depth1=5, depth2=4, depth3=2, initial_size=8, dim=768, heads=4, mlp_ratio=4, drop_rate=0.):#,device=device):\n        super(Generator, self).__init__()\n\n        #self.device = device\n        self.initial_size = initial_size\n        self.dim = dim\n        self.depth1 = depth1\n        self.depth2 = depth2\n        self.depth3 = depth3\n        self.heads = heads\n        self.mlp_ratio = mlp_ratio\n        self.droprate_rate =drop_rate\n        self.patches = ImgPatches(3, dim, 4)\n\n        self.mlp = nn.Linear(1024, (self.initial_size ** 2) * self.dim)\n\n        self.positional_embedding_1 = nn.Parameter(torch.zeros(1, (16**2), 768))\n        self.positional_embedding_2 = nn.Parameter(torch.zeros(1, (16*2)**2, 768//4))\n        self.positional_embedding_3 = nn.Parameter(torch.zeros(1, (16*4)**2, 768//16))\n\n        self.TransformerEncoder_encoder1 = TransformerEncoder(depth=self.depth1, dim=self.dim,heads=self.heads, mlp_ratio=self.mlp_ratio, drop_rate=self.droprate_rate)\n        self.TransformerEncoder_encoder2 = TransformerEncoder(depth=self.depth2, dim=self.dim//4, heads=self.heads, mlp_ratio=self.mlp_ratio, drop_rate=self.droprate_rate)\n        self.TransformerEncoder_encoder3 = TransformerEncoder(depth=self.depth3, dim=self.dim//16, heads=self.heads, mlp_ratio=self.mlp_ratio, drop_rate=self.droprate_rate)\n\n\n        self.linear = nn.Sequential(nn.Conv2d(self.dim//16, 3, 1, 1, 0))\n\n    def forward(self, img):\n        x = self.patches(img)\n\n        x = x + self.positional_embedding_1\n        H, W = self.initial_size, self.initial_size\n        x = self.TransformerEncoder_encoder1(x)\n        \n        \n\n        x,H,W = UpSampling(x,H,W) \n        x = x + self.positional_embedding_2\n        x = self.TransformerEncoder_encoder2(x)\n\n        x,H,W = UpSampling(x,H,W)\n        x = x + self.positional_embedding_3\n\n        x = self.TransformerEncoder_encoder3(x)\n        x = self.linear(x.permute(0, 2, 1).view(-1, self.dim//16, H, W))\n\n        return x\n\nclass Discriminator(nn.Module):\n    def __init__(self, diff_aug, image_size=64, patch_size=4, input_channel=3, num_classes=1,\n                 dim=768, depth=7, heads=4, mlp_ratio=4,\n                 drop_rate=0.):\n        super().__init__()\n        if image_size % patch_size != 0:\n            raise ValueError('Image size must be divisible by patch size.')\n        num_patches = (image_size//patch_size) ** 2\n        self.diff_aug = diff_aug\n        self.patch_size = patch_size\n        self.depth = depth\n        # Image patches and embedding layer\n        self.patches = ImgPatches(input_channel, dim, self.patch_size)\n\n        # Embedding for patch position and class\n        self.positional_embedding = nn.Parameter(torch.zeros(1, 65, dim))\n        self.class_embedding = nn.Parameter(torch.zeros(1, 1, dim))\n        nn.init.trunc_normal_(self.positional_embedding, std=0.2)\n        nn.init.trunc_normal_(self.class_embedding, std=0.2)\n\n        self.droprate = nn.Dropout(p=drop_rate)\n        self.TransfomerEncoder = TransformerEncoder(depth, dim, heads,\n                                      mlp_ratio, drop_rate)\n        self.norm = nn.LayerNorm(dim)\n        self.out = nn.Linear(dim, num_classes)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def forward(self, x):\n        x = DiffAugment(x, self.diff_aug)\n        b = x.shape[0]\n        # print(x.shape)\n        cls_token = self.class_embedding.expand(b, -1, -1)\n\n        x = self.patches(x)\n        x = torch.cat((cls_token, x), dim=1)\n        # print(self.patch_size)\n        # print(self.positional_embedding.shape)\n        x += self.positional_embedding\n        x = self.droprate(x)\n        x = self.TransfomerEncoder(x)\n        x = self.norm(x)\n        x = self.out(x[:, 0])\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:40:45.247532Z","iopub.execute_input":"2021-11-17T09:40:45.248078Z","iopub.status.idle":"2021-11-17T09:40:45.29495Z","shell.execute_reply.started":"2021-11-17T09:40:45.248038Z","shell.execute_reply":"2021-11-17T09:40:45.294054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    dev = \"cuda:0\"\nelse:\n    dev = \"cpu\"\n\ndevice = torch.device(dev)\n\ngenerator= Generator(depth1=5, depth2=4, depth3=2, initial_size=8, dim=768, heads=4, mlp_ratio=4, drop_rate=0.5)#,device = device)\ngenerator.to(device)\n\ndiscriminator = Discriminator(diff_aug = diff_aug, image_size=64, patch_size=4, input_channel=3, num_classes=1,\n                 dim=768, depth=7, heads=4, mlp_ratio=4,\n                 drop_rate=0.5)\ndiscriminator.to(device)\n\n\ngenerator.apply(inits_weight)\ndiscriminator.apply(inits_weight)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:40:45.296612Z","iopub.execute_input":"2021-11-17T09:40:45.297192Z","iopub.status.idle":"2021-11-17T09:40:50.693619Z","shell.execute_reply.started":"2021-11-17T09:40:45.297152Z","shell.execute_reply":"2021-11-17T09:40:50.692904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Custom code to resume training from the previous checkpoint \n'''\nwhole_model_path = '/kaggle/input/modelcheckpoint125/checkpoint.pth125'\nnets = torch.load(whole_model_path)\nprint(\"Loaded Model Checkpoint 130\")\nnetG_state_dict, netD_state_dict = nets['generator_state_dict'], nets['discriminator_state_dict']\n\nif torch.cuda.is_available():\n    dev = \"cuda:0\"\nelse:\n    dev = \"cpu\"\n\ndevice = torch.device(dev)\n\ngenerator= Generator()\ngenerator.load_state_dict(netG_state_dict)\ngenerator.to(device)\n\ndiscriminator = Discriminator(diff_aug = diff_aug)\ndiscriminator.load_state_dict(netD_state_dict)\ndiscriminator.to(device)\n\n'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if optimizer == 'Adam':\n    optim_gen = optim.Adam(filter(lambda p: p.requires_grad, generator.parameters()), lr=lr_gen, betas=(beta1, beta2))\n\n    optim_dis = optim.Adam(filter(lambda p: p.requires_grad, discriminator.parameters()),lr=lr_dis, betas=(beta1, beta2))\nelif optimizer == 'SGD':\n    optim_gen = optim.SGD(filter(lambda p: p.requires_grad, generator.parameters()),\n                lr=lr_gen, momentum=0.9)\n\n    optim_dis = optim.SGD(filter(lambda p: p.requires_grad, discriminator.parameters()),\n                lr=lr_dis, momentum=0.9)\n\nelif optimizer == 'RMSprop':\n    optim_gen = optim.RMSprop(filter(lambda p: p.requires_grad, discriminator.parameters()),\n                lr=lr_dis, eps=1e-08, weight_decay=weight_decay, momentum=0, centered=False)\n\n    optim_dis = optim.RMSprop(filter(lambda p: p.requires_grad, discriminator.parameters()), lr=lr_dis, eps=1e-08, weight_decay=weight_decay, momentum=0, centered=False)\n\ngen_scheduler = LinearLrDecay(optim_gen, lr_gen, 0.0, 0, max_iter * n_critic)\ndis_scheduler = LinearLrDecay(optim_dis, lr_dis, 0.0, 0, max_iter * n_critic)\n\nprint(\"optimizer:\",optimizer)\n\nfid_stat = 'fid_stat/fid_stats_cifar10_train.npz'\n\nwriter=SummaryWriter()\nwriter_dict = {'writer':writer}\nwriter_dict[\"train_global_steps\"]=0\nwriter_dict[\"valid_global_steps\"]=0","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:40:50.69492Z","iopub.execute_input":"2021-11-17T09:40:50.695681Z","iopub.status.idle":"2021-11-17T09:40:50.716744Z","shell.execute_reply.started":"2021-11-17T09:40:50.695636Z","shell.execute_reply":"2021-11-17T09:40:50.715898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GeneratorLoss = []\nDiscriminatorLoss = []","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:40:50.718149Z","iopub.execute_input":"2021-11-17T09:40:50.718642Z","iopub.status.idle":"2021-11-17T09:40:50.727755Z","shell.execute_reply.started":"2021-11-17T09:40:50.71859Z","shell.execute_reply":"2021-11-17T09:40:50.7269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_gradient_penalty(D, real_samples, fake_samples, phi):\n    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n    alpha = torch.Tensor(np.random.random((real_samples.size(0), 1, 1, 1))).to(real_samples.get_device())\n    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n    d_interpolates = D(interpolates)\n    fake = torch.ones([real_samples.shape[0], 1], requires_grad=False).to(real_samples.get_device())\n    gradients = torch.autograd.grad(\n        outputs=d_interpolates,\n        inputs=interpolates,\n        grad_outputs=fake,\n        create_graph=True,\n        retain_graph=True,\n        only_inputs=True,\n    )[0]\n    gradients = gradients.contiguous().view(gradients.size(0), -1)\n    gradient_penalty = ((gradients.norm(2, dim=1) - phi) ** 2).mean()\n    return gradient_penalty\n\n\ndef train(noise,generator, discriminator, optim_gen, optim_dis,\n        epoch, writer, schedulers, img_size=64, latent_dim = latent_dim,\n        n_critic = n_critic,\n        gener_batch_size=gener_batch_size, device=\"cuda:0\"):\n\n\n    writer = writer_dict['writer']\n    gen_step = 0\n\n    generator = generator.train()\n    discriminator = discriminator.train()\n\n    transform = transforms.Compose(\n        [\n            transforms.Resize(size=(64, 64)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n    \n    train_set = InpaintDataset(TrainB, TrainB, transform=transform)\n\n    train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=10, shuffle=True)\n    total_gener_loss = 0\n    total_dis_loss = 0\n    \n    !mkdir output_imgs/{epoch}/\n    !mkdir generated_imgs/{epoch}/\n    !mkdir real_imgs/{epoch}/\n    !mkdir fake_imgs/{epoch}/\n    !mkdir real_test_imgs/{epoch}/\n\n    for index, (real_imgs, mask_imgs) in enumerate(train_loader):\n\n        global_steps = writer_dict['train_global_steps']\n\n        real_imgs = real_imgs.type(torch.cuda.FloatTensor)\n    \n        mask_imgs = mask_imgs.type(torch.cuda.FloatTensor)\n        optim_dis.zero_grad()\n        real_valid = discriminator(real_imgs)\n        fake_imgs = generator(mask_imgs).detach()\n        \n        fake_valid = discriminator(fake_imgs)\n\n        if loss == 'hinge':\n            loss_dis = torch.mean(nn.ReLU(inplace=True)(1.0 - real_valid)).to(device) + torch.mean(nn.ReLU(inplace=True)(1 + fake_valid)).to(device)\n        elif loss == 'wgangp_eps':\n            gradient_penalty = compute_gradient_penalty(discriminator, real_imgs, fake_imgs.detach(), phi)\n            loss_dis = -torch.mean(real_valid) + torch.mean(fake_valid) + gradient_penalty * 10 / (phi ** 2)        \n\n        loss_dis.backward()\n        optim_dis.step()\n\n        writer.add_scalar(\"loss_dis\", loss_dis.item(), global_steps)\n\n        if global_steps % n_critic == 0:\n\n            optim_gen.zero_grad()\n            if schedulers:\n                gen_scheduler, dis_scheduler = schedulers\n                g_lr = gen_scheduler.step(global_steps)\n                d_lr = dis_scheduler.step(global_steps)\n                writer.add_scalar('LR/g_lr', g_lr, global_steps)\n                writer.add_scalar('LR/d_lr', d_lr, global_steps)\n\n            gener_noise = mask_imgs\n            gener_noise = gener_noise.type(torch.cuda.FloatTensor)\n\n            generated_imgs= generator(gener_noise)\n            fake_valid = discriminator(generated_imgs)\n\n            gener_loss = -torch.mean(fake_valid).to(device)\n            gener_loss.backward()\n            optim_gen.step()\n            writer.add_scalar(\"gener_loss\", gener_loss.item(), global_steps)\n\n            gen_step += 1\n            total_gener_loss += gener_loss.item()\n            total_dis_loss += loss_dis.item()\n            \n\n        if epoch % 1 == 0:\n            for ind in range(real_imgs.shape[0]):\n                grid_imgs = [real_imgs[ind],  mask_imgs[ind], generated_imgs[ind]]\n                save_image(grid_imgs, f'/kaggle/working/output_imgs/{epoch}/output_img_{(index * 30) + ind}.jpg',nrow=3, normalize=True, scale_each=True)\n                save_image(real_imgs[ind], f'/kaggle/working/real_imgs/{epoch}/{(index * 30) + ind}.jpg',nrow=1, normalize=True, scale_each=True)\n                save_image(generated_imgs[ind], f'/kaggle/working/generated_imgs/{epoch}/{(index * 30) + ind}.jpg',nrow=1, normalize=True, scale_each=True)\n\n\n        if gen_step and index % 100 == 0:\n            actual_imgs = mask_imgs[:25]\n            sample_imgs = generated_imgs[:25]\n            save_image(sample_imgs, f'/kaggle/working/generated_imgs/generated_img_{epoch}_{index % len(train_loader)}.jpg', nrow=5, normalize=True, scale_each=True)            \n            \n            tqdm.write(\"[Epoch %d] [Batch %d/%d] [D loss: %f] [G loss: %f]\" %\n                (epoch+1, index % len(train_loader), len(train_loader), loss_dis.item(), gener_loss.item()))\n    \n    GeneratorLoss.append(total_gener_loss/len(train_loader))\n    DiscriminatorLoss.append(total_dis_loss/len(train_loader))","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:40:50.731747Z","iopub.execute_input":"2021-11-17T09:40:50.731999Z","iopub.status.idle":"2021-11-17T09:40:50.839494Z","shell.execute_reply.started":"2021-11-17T09:40:50.731972Z","shell.execute_reply":"2021-11-17T09:40:50.838752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evaluation of the model using FID score and SSIM score**","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/evaluation/evaluation')\nfrom inception_score.inception_score import inception_score\nfrom fid.fid import calculate_fid_given_paths\nfrom ssim.ssim import ssim\nfrom psnr.psnr import psnr\nimport os\nimport numpy as np\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms\nimport cv2\n\"\"\"\ndef inception_score(imgs, cuda=True, batch_size=64, resize=False, splits=1):\ndef calculate_fid_given_paths(paths, batch_size, cuda, dims):\ndef ssim(img1, img2, window_size = 11, size_average = True):\n\"\"\"\nSIZE = (64,64)\n_transforms_fun = transforms.Compose([transforms.Resize((64,64)), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\ndef _inception_score(path, cuda=True, batch_size=1, resize=True, splits=1):\n    imgs = []\n    for file in os.listdir(path):\n        if file.endswith(\"png\"):\n            img = Image.open(os.path.join(path, file)).convert(\"RGB\")\n            #print(np.array(img).shape)\n            imgs.append(_transforms_fun(img))\n    imgs = torch.stack(imgs)\n    #print(imgs.size())\n    return inception_score(imgs, cuda, batch_size, resize, splits)\n\ndef _fid(paths, batch_size=100, cuda=True, dims=768):\n    return calculate_fid_given_paths(paths, batch_size, cuda, dims)\n\ndef _ssim(paths, window_size=11, size_average=True):\n    path1, path2 = paths\n    imgs1, imgs2 = [], []\n    batch_size = 10\n    j = 0\n    total = 0\n    ssim_score = 0\n    for file in os.listdir(path1):\n        if file.endswith(\"jpg\"):\n            img1 = Image.open(os.path.join(path1, file)).convert(\"RGB\")\n            img2 = Image.open(os.path.join(path2, file)).convert(\"RGB\")\n\n            imgs1.append(_transforms_fun(img1))\n            imgs2.append(_transforms_fun(img2))\n            j = j + 1\n            total = total + 1\n        if j == batch_size - 1:\n            imgs1 = torch.stack(imgs1)\n            imgs2 = torch.stack(imgs2)\n            ssim_score = ssim_score + batch_size * ssim(imgs1, imgs2, window_size = 11, size_average = True)\n            imgs1, imgs2 = [], []\n            j = 0\n    if j != 0:\n        imgs1 = torch.stack(imgs1)\n        imgs2 = torch.stack(imgs2)\n        ssim_score = ssim_score +  (j+1) * ssim(imgs1, imgs2, window_size = 11, size_average = True)\n    return ssim_score / total\n\ndef _psnr(paths):\n    path1, path2 = paths\n    imgs1, imgs2 = [], []\n    psnr_value = 0\n    num = 1\n    for file in os.listdir(path1):\n        if file.endswith(\"png\"):\n            img1 = Image.open(os.path.join(path1, file)).convert(\"RGB\")\n            img2 = Image.open(os.path.join(path2, file)).convert(\"RGB\")\n            psnr_value = psnr_value + psnr(cv2.resize(np.array(img1),SIZE), cv2.resize(np.array(img2), SIZE))\n            num = num + 1\n\n    return psnr_value / num\n\ndef _meanl1(paths):\n    path1, path2 = paths\n    imgs1, imgs2 = [], []\n    total_error = 0\n    num = 1\n    for file in os.listdir(path1):\n        if file.endswith(\"png\"):\n            img1 = Image.open(os.path.join(path1, file)).convert(\"RGB\")\n            img2 = Image.open(os.path.join(path2, file)).convert(\"RGB\")\n\n            l1_error = np.mean(np.abs(cv2.resize(np.array(img1),SIZE)-cv2.resize(np.array(img2), SIZE)))\n            #print(np.array(img1).shape, l1_error,np.sum(np.abs(cv2.resize(np.array(img1),SIZE)-cv2.resize(np.array(img2), SIZE)))/256/256/3)\n            total_error = total_error + l1_error\n            num = num + 1\n\n    return total_error / num\n\nmetrics = {\"is\":_inception_score, \"fid\":_fid, \"ssim\":_ssim, \"psnr\":_psnr, \"meanl1\":_meanl1}","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:40:50.841205Z","iopub.execute_input":"2021-11-17T09:40:50.841753Z","iopub.status.idle":"2021-11-17T09:40:51.58747Z","shell.execute_reply.started":"2021-11-17T09:40:50.841715Z","shell.execute_reply":"2021-11-17T09:40:51.586605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fid_scores = []\nssim_scores = []","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:40:51.591006Z","iopub.execute_input":"2021-11-17T09:40:51.591597Z","iopub.status.idle":"2021-11-17T09:40:51.597481Z","shell.execute_reply.started":"2021-11-17T09:40:51.591562Z","shell.execute_reply":"2021-11-17T09:40:51.596622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(generator, epoch):\n    generator = generator.eval()\n    validation(generator, epoch)\n    score = os.popen(f'python -m pytorch_fid /kaggle/working/real_test_imgs/{epoch} /kaggle/working/fake_imgs/{epoch}').read()\n    fid_score = float(score.split()[1])\n    ssim_score = metrics['ssim']([f'/kaggle/working/real_test_imgs/{epoch}', f'/kaggle/working/fake_imgs/{epoch}'])\n    print(f\"FID score: {fid_score}\")\n    return fid_score, ssim_score\n\ndef validation(generator, epoch):\n    transform = transforms.Compose(\n        [\n            transforms.Resize(size=(64, 64)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n    \n    test_set = InpaintDataset(TestB, TestB, transform=transform)\n\n    test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=30, shuffle=True)\n    \n    for index, (real_imgs, mask_imgs) in enumerate(test_loader):\n\n        real_imgs = real_imgs.type(torch.cuda.FloatTensor)\n    \n        mask_imgs = mask_imgs.type(torch.cuda.FloatTensor)\n        fake_imgs = generator(mask_imgs).detach()        \n        for ind in range(real_imgs.shape[0]):\n            save_image(real_imgs[ind], f'/kaggle/working/real_test_imgs/{epoch}/{(index * 30) + ind}.jpg',nrow=1, normalize=True, scale_each=True)\n            save_image(fake_imgs[ind], f'/kaggle/working/fake_imgs/{epoch}/{(index * 30) + ind}.jpg',nrow=1, normalize=True, scale_each=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:40:51.600821Z","iopub.execute_input":"2021-11-17T09:40:51.603202Z","iopub.status.idle":"2021-11-17T09:40:51.613318Z","shell.execute_reply.started":"2021-11-17T09:40:51.603167Z","shell.execute_reply":"2021-11-17T09:40:51.612525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best = 1e4\n\nfor epoch in range(50):\n    print(\"Epoch {}\".format(epoch+1))\n\n    lr_schedulers = (gen_scheduler, dis_scheduler) if lr_decay else None\n\n    train(noise, generator, discriminator, optim_gen, optim_dis,\n    epoch, writer, lr_schedulers,img_size = 64, latent_dim = latent_dim,\n    n_critic = n_critic,\n    gener_batch_size = gener_batch_size)\n\n    checkpoint = {'epoch': epoch, 'best_fid': best}\n    checkpoint['generator_state_dict'] = generator.state_dict()\n    checkpoint['discriminator_state_dict'] = discriminator.state_dict()\n    \n    fid_score, ssim_score = validate(generator, epoch)\n\n    print(ssim_score.item())\n    fid_scores.append(fid_score)\n    ssim_scores.append(ssim_score.item())\n    \n    if epoch % 5 != 0:\n        os.system(f'rm -rf /kaggle/working/output_imgs/{epoch}')\n        os.system(f'rm -rf /kaggle/working/real_imgs/{epoch}')\n        os.system(f'rm -rf /kaggle/working/generated_imgs/{epoch}')\n        os.system(f'rm -rf /kaggle/working/fake_imgs/{epoch}')\n        os.system(f'rm -rf /kaggle/working/real_test_imgs/{epoch}')\n        print(\"Deleted files at epoch {}\".format(epoch + 1))\n\n    if fid_score < best:\n        save_checkpoint(checkpoint,  is_best=(fid_score < best), epoch=epoch, output_dir=output_dir)\n        best = fid_score\n        print(\"Saved Latest Model!\")\n\n\ncheckpoint = {'epoch':epoch, 'best_fid':best}\ncheckpoint['generator_state_dict'] = generator.state_dict()\ncheckpoint['discriminator_state_dict'] = discriminator.state_dict()","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:40:51.615614Z","iopub.execute_input":"2021-11-17T09:40:51.616613Z","iopub.status.idle":"2021-11-17T09:41:00.437965Z","shell.execute_reply.started":"2021-11-17T09:40:51.616569Z","shell.execute_reply":"2021-11-17T09:41:00.43657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fid_scores","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:41:00.439606Z","iopub.status.idle":"2021-11-17T09:41:00.440462Z","shell.execute_reply.started":"2021-11-17T09:41:00.440143Z","shell.execute_reply":"2021-11-17T09:41:00.4402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ssim_scores","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:41:00.441873Z","iopub.status.idle":"2021-11-17T09:41:00.442568Z","shell.execute_reply.started":"2021-11-17T09:41:00.442284Z","shell.execute_reply":"2021-11-17T09:41:00.442311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GeneratorLoss","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:41:00.44394Z","iopub.status.idle":"2021-11-17T09:41:00.444646Z","shell.execute_reply.started":"2021-11-17T09:41:00.444383Z","shell.execute_reply":"2021-11-17T09:41:00.444408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DiscriminatorLoss","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:41:00.446019Z","iopub.status.idle":"2021-11-17T09:41:00.446771Z","shell.execute_reply.started":"2021-11-17T09:41:00.446456Z","shell.execute_reply":"2021-11-17T09:41:00.446483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(1, len(GeneratorLoss) + 1), GeneratorLoss)\nplt.savefig('GeneratorLoss.png')","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:41:00.448088Z","iopub.status.idle":"2021-11-17T09:41:00.448766Z","shell.execute_reply.started":"2021-11-17T09:41:00.448493Z","shell.execute_reply":"2021-11-17T09:41:00.448518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(1, len(DiscriminatorLoss) + 1), DiscriminatorLoss)\nplt.savefig('DiscriminatorLoss.png')","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:41:00.450085Z","iopub.status.idle":"2021-11-17T09:41:00.450777Z","shell.execute_reply.started":"2021-11-17T09:41:00.450496Z","shell.execute_reply":"2021-11-17T09:41:00.450539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip output_imgs.zip output_imgs/*\n!zip generated_imgs.zip generated_imgs/*\n!zip checkpoints.zip checkpoint/*","metadata":{"execution":{"iopub.status.busy":"2021-11-17T09:41:00.452034Z","iopub.status.idle":"2021-11-17T09:41:00.45266Z","shell.execute_reply.started":"2021-11-17T09:41:00.452411Z","shell.execute_reply":"2021-11-17T09:41:00.452447Z"},"trusted":true},"execution_count":null,"outputs":[]}]}