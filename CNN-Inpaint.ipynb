{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inpaint.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NagaHarshita/BTP-ImageInpainting/blob/main/CNN-Inpaint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN8E2hIZpVqM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27bc02a2-561e-4508-ca5e-32498b70fc4e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "we47-EvF1tTS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47eb5740-26cd-4dd6-dce4-513bff81f186"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lavizcRzvTEe"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "import skimage\n",
        "\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.nn import Parameter\n",
        "\n",
        "import pickle as pkl\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1PwQX7DoQdO"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCRNIrtzlGwF"
      },
      "source": [
        "def init_weights(net, init_type='normal', gain=0.02):\n",
        "    from torch.nn import init\n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
        "            if init_type == 'normal':\n",
        "                init.normal(m.weight.data, 0.0, gain)\n",
        "            elif init_type == 'xavier':\n",
        "                init.xavier_normal(m.weight.data, gain=gain)\n",
        "            elif init_type == 'kaiming':\n",
        "                init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n",
        "            elif init_type == 'orthogonal':\n",
        "                init.orthogonal(m.weight.data, gain=gain)\n",
        "            else:\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\n",
        "                init.constant(m.bias.data, 0.0)\n",
        "        elif classname.find('BatchNorm2d') != -1:\n",
        "            init.normal(m.weight.data, 1.0, gain)\n",
        "            init.constant(m.bias.data, 0.0)\n",
        "\n",
        "    print('initialize network with %s' % init_type)\n",
        "    net.apply(init_func)\n",
        "\n",
        "\n",
        "def get_pad(in_,  ksize, stride, atrous=1):\n",
        "    out_ = np.ceil(float(in_)/stride)\n",
        "    return int(((out_ - 1) * stride + atrous*(ksize-1) + 1 - in_)/2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xicdo-b8lsPa"
      },
      "source": [
        "class GatedConv2dWithActivation(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Gated Convlution layer with activation (default activation:LeakyReLU)\n",
        "    Params: same as conv2d\n",
        "    Input: The feature from last layer \"I\"\n",
        "    Output:\\phi(f(I))*\\sigmoid(g(I))\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True,batch_norm=True, activation=torch.nn.LeakyReLU(0.2, inplace=True)):\n",
        "        super(GatedConv2dWithActivation, self).__init__()\n",
        "        self.batch_norm = batch_norm\n",
        "        self.activation = activation\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
        "        self.mask_conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
        "        self.batch_norm2d = torch.nn.BatchNorm2d(out_channels)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "    def gated(self, mask):\n",
        "        #return torch.clamp(mask, -1, 1)\n",
        "        return self.sigmoid(mask)\n",
        "    def forward(self, input):\n",
        "        x = self.conv2d(input)\n",
        "        mask = self.mask_conv2d(input)\n",
        "        if self.activation is not None:\n",
        "            x = self.activation(x) * self.gated(mask)\n",
        "        else:\n",
        "            x = x * self.gated(mask)\n",
        "        if self.batch_norm:\n",
        "            return self.batch_norm2d(x)\n",
        "        else:\n",
        "            return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK8f2-1VlvXz"
      },
      "source": [
        "class GatedDeConv2dWithActivation(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Gated DeConvlution layer with activation (default activation:LeakyReLU)\n",
        "    resize + conv\n",
        "    Params: same as conv2d\n",
        "    Input: The feature from last layer \"I\"\n",
        "    Output:\\phi(f(I))*\\sigmoid(g(I))\n",
        "    \"\"\"\n",
        "    def __init__(self, scale_factor, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, batch_norm=True,activation=torch.nn.LeakyReLU(0.2, inplace=True)):\n",
        "        super(GatedDeConv2dWithActivation, self).__init__()\n",
        "        self.conv2d = GatedConv2dWithActivation(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, batch_norm, activation)\n",
        "        self.scale_factor = scale_factor\n",
        "\n",
        "    def forward(self, input):\n",
        "        #print(input.size())\n",
        "        x = F.interpolate(input, scale_factor=2)\n",
        "        return self.conv2d(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD4JrJL2lvxq"
      },
      "source": [
        "class SNGatedConv2dWithActivation(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Gated Convolution with spetral normalization\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, batch_norm=True, activation=torch.nn.LeakyReLU(0.2, inplace=True)):\n",
        "        super(SNGatedConv2dWithActivation, self).__init__()\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
        "        self.mask_conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
        "        self.activation = activation\n",
        "        self.batch_norm = batch_norm\n",
        "        self.batch_norm2d = torch.nn.BatchNorm2d(out_channels)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        self.conv2d = torch.nn.utils.spectral_norm(self.conv2d)\n",
        "        self.mask_conv2d = torch.nn.utils.spectral_norm(self.mask_conv2d)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "\n",
        "    def gated(self, mask):\n",
        "        return self.sigmoid(mask)\n",
        "        #return torch.clamp(mask, -1, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv2d(input)\n",
        "        mask = self.mask_conv2d(input)\n",
        "        if self.activation is not None:\n",
        "            x = self.activation(x) * self.gated(mask)\n",
        "        else:\n",
        "            x = x * self.gated(mask)\n",
        "        if self.batch_norm:\n",
        "            return self.batch_norm2d(x)\n",
        "        else:\n",
        "            return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTUGSWkQl4gU"
      },
      "source": [
        "class SNGatedDeConv2dWithActivation(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Gated DeConvlution layer with activation (default activation:LeakyReLU)\n",
        "    resize + conv\n",
        "    Params: same as conv2d\n",
        "    Input: The feature from last layer \"I\"\n",
        "    Output:\\phi(f(I))*\\sigmoid(g(I))\n",
        "    \"\"\"\n",
        "    def __init__(self, scale_factor, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, batch_norm=True, activation=torch.nn.LeakyReLU(0.2, inplace=True)):\n",
        "        super(SNGatedDeConv2dWithActivation, self).__init__()\n",
        "        self.conv2d = SNGatedConv2dWithActivation(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, batch_norm, activation)\n",
        "        self.scale_factor = scale_factor\n",
        "\n",
        "    def forward(self, input):\n",
        "        #print(input.size())\n",
        "        x = F.interpolate(input, scale_factor=2)\n",
        "        return self.conv2d(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5zwWQLXl7uZ"
      },
      "source": [
        "class SNConvWithActivation(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    SN convolution for spetral normalization conv\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, activation=torch.nn.LeakyReLU(0.2, inplace=True)):\n",
        "        super(SNConvWithActivation, self).__init__()\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
        "        self.conv2d = torch.nn.utils.spectral_norm(self.conv2d)\n",
        "        self.activation = activation\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "    def forward(self, input):\n",
        "        x = self.conv2d(input)\n",
        "        if self.activation is not None:\n",
        "            return self.activation(x)\n",
        "        else:\n",
        "          x = x * self.gated(mask)\n",
        "        if self.batch_norm:\n",
        "            return self.batch_norm2d(x)\n",
        "        else:\n",
        "            return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fzwr4pGRmJM4"
      },
      "source": [
        "class SNGatedDeConv2dWithActivation(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Gated DeConvlution layer with activation (default activation:LeakyReLU)\n",
        "    resize + conv\n",
        "    Params: same as conv2d\n",
        "    Input: The feature from last layer \"I\"\n",
        "    Output:\\phi(f(I))*\\sigmoid(g(I))\n",
        "    \"\"\"\n",
        "    def __init__(self, scale_factor, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, batch_norm=True, activation=torch.nn.LeakyReLU(0.2, inplace=True)):\n",
        "        super(SNGatedDeConv2dWithActivation, self).__init__()\n",
        "        self.conv2d = SNGatedConv2dWithActivation(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, batch_norm, activation)\n",
        "        self.scale_factor = scale_factor\n",
        "\n",
        "    def forward(self, input):\n",
        "        #print(input.size())\n",
        "        x = F.interpolate(input, scale_factor=2)\n",
        "        return self.conv2d(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ds_An4bmL_m"
      },
      "source": [
        "class SNConvWithActivation(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    SN convolution for spetral normalization conv\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, activation=torch.nn.LeakyReLU(0.2, inplace=True)):\n",
        "        super(SNConvWithActivation, self).__init__()\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
        "        self.conv2d = torch.nn.utils.spectral_norm(self.conv2d)\n",
        "        self.activation = activation\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "    def forward(self, input):\n",
        "        x = self.conv2d(input)\n",
        "        if self.activation is not None:\n",
        "            return self.activation(x)\n",
        "        else:\n",
        "            return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOwQo-97mTUy"
      },
      "source": [
        "def l2normalize(v, eps=1e-12):\n",
        "    return v / (v.norm() + eps)\n",
        "\n",
        "\n",
        "class SpectralNorm(nn.Module):\n",
        "    def __init__(self, module, name='weight', power_iterations=1):\n",
        "        super(SpectralNorm, self).__init__()\n",
        "        self.module = module\n",
        "        self.name = name\n",
        "        self.power_iterations = power_iterations\n",
        "        if not self._made_params():\n",
        "            self._make_params()\n",
        "\n",
        "    def _update_u_v(self):\n",
        "        u = getattr(self.module, self.name + \"_u\")\n",
        "        v = getattr(self.module, self.name + \"_v\")\n",
        "        w = getattr(self.module, self.name + \"_bar\")\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        for _ in range(self.power_iterations):\n",
        "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
        "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
        "\n",
        "        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n",
        "        sigma = u.dot(w.view(height, -1).mv(v))\n",
        "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
        "\n",
        "    def _made_params(self):\n",
        "        try:\n",
        "            u = getattr(self.module, self.name + \"_u\")\n",
        "            v = getattr(self.module, self.name + \"_v\")\n",
        "            w = getattr(self.module, self.name + \"_bar\")\n",
        "            return True\n",
        "        except AttributeError:\n",
        "            return False\n",
        "\n",
        "\n",
        "    def _make_params(self):\n",
        "        w = getattr(self.module, self.name)\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        width = w.view(height, -1).data.shape[1]\n",
        "\n",
        "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
        "        u.data = l2normalize(u.data)\n",
        "        v.data = l2normalize(v.data)\n",
        "        w_bar = Parameter(w.data)\n",
        "\n",
        "        del self.module._parameters[self.name]\n",
        "\n",
        "        self.module.register_parameter(self.name + \"_u\", u)\n",
        "        self.module.register_parameter(self.name + \"_v\", v)\n",
        "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
        "\n",
        "\n",
        "    def forward(self, *args):\n",
        "        self._update_u_v()\n",
        "        return self.module.forward(*args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFqe_nqXmn02"
      },
      "source": [
        "class Self_Attn(nn.Module):\n",
        "    \"\"\" Self attention Layer\"\"\"\n",
        "    def __init__(self,in_dim,activation,with_attn=False):\n",
        "        super(Self_Attn,self).__init__()\n",
        "        self.chanel_in = in_dim\n",
        "        self.activation = activation\n",
        "        self.with_attn = with_attn\n",
        "        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
        "        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
        "        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "        self.softmax  = nn.Softmax(dim=-1) #\n",
        "    def forward(self,x):\n",
        "        \"\"\"\n",
        "            inputs :\n",
        "                x : input feature maps( B X C X W X H)\n",
        "            returns :\n",
        "                out : self attention value + input feature\n",
        "                attention: B X N X N (N is Width*Height)\n",
        "        \"\"\"\n",
        "        m_batchsize,C,width ,height = x.size()\n",
        "        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X CX(N)\n",
        "        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)\n",
        "        energy =  torch.bmm(proj_query,proj_key) # transpose check\n",
        "        attention = self.softmax(energy) # BX (N) X (N)\n",
        "        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n",
        "\n",
        "        out = torch.bmm(proj_value,attention.permute(0,2,1) )\n",
        "        out = out.view(m_batchsize,C,width,height)\n",
        "\n",
        "        out = self.gamma*out + x\n",
        "        if self.with_attn:\n",
        "            return out ,attention\n",
        "        else:\n",
        "            return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gmkm6RREm7Hg"
      },
      "source": [
        "class SAGenerator(nn.Module):\n",
        "    \"\"\"Generator.\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, image_size=64, z_dim=100, conv_dim=64):\n",
        "        super(Generator, self).__init__()\n",
        "        self.imsize = image_size\n",
        "        layer1 = []\n",
        "        layer2 = []\n",
        "        layer3 = []\n",
        "        last = []\n",
        "\n",
        "        repeat_num = int(np.log2(self.imsize)) - 3\n",
        "        mult = 2 ** repeat_num # 8\n",
        "        layer1.append(SpectralNorm(nn.ConvTranspose2d(z_dim, conv_dim * mult, 4)))\n",
        "        layer1.append(nn.BatchNorm2d(conv_dim * mult))\n",
        "        layer1.append(nn.ReLU())\n",
        "\n",
        "        curr_dim = conv_dim * mult\n",
        "\n",
        "        layer2.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
        "        layer2.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
        "        layer2.append(nn.ReLU())\n",
        "\n",
        "        curr_dim = int(curr_dim / 2)\n",
        "\n",
        "        layer3.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
        "        layer3.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
        "        layer3.append(nn.ReLU())\n",
        "\n",
        "        if self.imsize == 64:\n",
        "            layer4 = []\n",
        "            curr_dim = int(curr_dim / 2)\n",
        "            layer4.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
        "            layer4.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
        "            layer4.append(nn.ReLU())\n",
        "            self.l4 = nn.Sequential(*layer4)\n",
        "            curr_dim = int(curr_dim / 2)\n",
        "\n",
        "        self.l1 = nn.Sequential(*layer1)\n",
        "        self.l2 = nn.Sequential(*layer2)\n",
        "        self.l3 = nn.Sequential(*layer3)\n",
        "\n",
        "        last.append(nn.ConvTranspose2d(curr_dim, 3, 4, 2, 1))\n",
        "        last.append(nn.Tanh())\n",
        "        self.last = nn.Sequential(*last)\n",
        "\n",
        "        self.attn1 = Self_Attn( 128, 'relu')\n",
        "        self.attn2 = Self_Attn( 64,  'relu')\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = z.view(z.size(0), z.size(1), 1, 1)\n",
        "        out=self.l1(z)\n",
        "        out=self.l2(out)\n",
        "        out=self.l3(out)\n",
        "        out,p1 = self.attn1(out)\n",
        "        out=self.l4(out)\n",
        "        out,p2 = self.attn2(out)\n",
        "        out=self.last(out)\n",
        "\n",
        "        return out, p1, p2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soshMUDfnDlC"
      },
      "source": [
        "class InpaintSADirciminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(InpaintSADirciminator, self).__init__()\n",
        "        cnum = 32\n",
        "        self.discriminator_net = nn.Sequential(\n",
        "            SNConvWithActivation(5, 2*cnum, 4, 2, padding=get_pad(256, 5, 2)),\n",
        "            SNConvWithActivation(2*cnum, 4*cnum, 4, 2, padding=get_pad(128, 5, 2)),\n",
        "            SNConvWithActivation(4*cnum, 8*cnum, 4, 2, padding=get_pad(64, 5, 2)),\n",
        "            SNConvWithActivation(8*cnum, 8*cnum, 4, 2, padding=get_pad(32, 5, 2)),\n",
        "            SNConvWithActivation(8*cnum, 8*cnum, 4, 2, padding=get_pad(16, 5, 2)),\n",
        "            SNConvWithActivation(8*cnum, 8*cnum, 4, 2, padding=get_pad(8, 5, 2)),\n",
        "            Self_Attn(8*cnum, 'relu'),\n",
        "            SNConvWithActivation(8*cnum, 8*cnum, 4, 2, padding=get_pad(4, 5, 2)),\n",
        "        )\n",
        "        self.linear = nn.Linear(8*cnum*2*2, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.discriminator_net(input)\n",
        "        x = x.view((x.size(0),-1))\n",
        "        #x = self.linear(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlqdMXpKnF0g"
      },
      "source": [
        "class SADiscriminator(nn.Module):\n",
        "    \"\"\"Discriminator, Auxiliary Classifier.\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size=64, image_size=64, conv_dim=64):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.imsize = image_size\n",
        "        layer1 = []\n",
        "        layer2 = []\n",
        "        layer3 = []\n",
        "        last = []\n",
        "\n",
        "        layer1.append(SpectralNorm(nn.Conv2d(3, conv_dim, 4, 2, 1)))\n",
        "        layer1.append(nn.LeakyReLU(0.1))\n",
        "\n",
        "        curr_dim = conv_dim\n",
        "\n",
        "        layer2.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
        "        layer2.append(nn.LeakyReLU(0.1))\n",
        "        curr_dim = curr_dim * 2\n",
        "\n",
        "        layer3.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
        "        layer3.append(nn.LeakyReLU(0.1))\n",
        "        curr_dim = curr_dim * 2\n",
        "\n",
        "        if self.imsize == 64:\n",
        "            layer4 = []\n",
        "            layer4.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
        "            layer4.append(nn.LeakyReLU(0.1))\n",
        "            self.l4 = nn.Sequential(*layer4)\n",
        "            curr_dim = curr_dim*2\n",
        "        self.l1 = nn.Sequential(*layer1)\n",
        "        self.l2 = nn.Sequential(*layer2)\n",
        "        self.l3 = nn.Sequential(*layer3)\n",
        "\n",
        "        last.append(nn.Conv2d(curr_dim, 1, 4))\n",
        "        self.last = nn.Sequential(*last)\n",
        "\n",
        "        self.attn1 = Self_Attn(256, 'relu')\n",
        "        self.attn2 = Self_Attn(512, 'relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.l1(x)\n",
        "        out = self.l2(out)\n",
        "        out = self.l3(out)\n",
        "        out,p1 = self.attn1(out)\n",
        "        out=self.l4(out)\n",
        "        out,p2 = self.attn2(out)\n",
        "        out=self.last(out)\n",
        "\n",
        "        return out.squeeze(), p1, p2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_OR2GGw3giK"
      },
      "source": [
        "## Image preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQw2pHpS3jQg"
      },
      "source": [
        "def transform_initialize(crop_size, config=['random_crop', 'to_tensor']):\n",
        "        \"\"\"\n",
        "        Initialize the transformation oprs and create transform function for img\n",
        "        \"\"\"\n",
        "        transforms_oprs = {}\n",
        "        transforms_oprs[\"hflip\"]= transforms.RandomHorizontalFlip(0.5)\n",
        "        transforms_oprs[\"vflip\"] = transforms.RandomVerticalFlip(0.5)\n",
        "        transforms_oprs[\"random_crop\"] = transforms.RandomCrop(crop_size)\n",
        "        transforms_oprs[\"to_tensor\"] = transforms.ToTensor()\n",
        "        transforms_oprs[\"norm\"] = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "        transforms_oprs[\"resize\"] = transforms.Resize(crop_size)\n",
        "        transforms_oprs[\"center_crop\"] = transforms.CenterCrop(crop_size)\n",
        "        transforms_oprs[\"rdresizecrop\"] = transforms.RandomResizedCrop(crop_size, scale=(0.7, 1.0), ratio=(1,1), interpolation=2)\n",
        "        transforms_fun = transforms.Compose([transforms_oprs[name] for name in config])\n",
        "        return transforms_fun\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM8yUzi7u5fz"
      },
      "source": [
        "## DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx7OQJ49vGg8"
      },
      "source": [
        "class BaseDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def __len__(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __getitem__(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def transform_initialize(self, crop_size, config=['random_crop', 'to_tensor', 'norm']):\n",
        "        \"\"\"\n",
        "        Initialize the transformation oprs and create transform function for img\n",
        "        \"\"\"\n",
        "        self.transforms_oprs = {}\n",
        "        self.transforms_oprs[\"hflip\"]= transforms.RandomHorizontalFlip(0.5)\n",
        "        self.transforms_oprs[\"vflip\"] = transforms.RandomVerticalFlip(0.5)\n",
        "        self.transforms_oprs[\"random_crop\"] = transforms.RandomCrop(crop_size)\n",
        "        self.transforms_oprs[\"to_tensor\"] = transforms.ToTensor()\n",
        "        self.transforms_oprs[\"norm\"] = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "        self.transforms_oprs[\"resize\"] = transforms.Resize(crop_size)\n",
        "        self.transforms_oprs[\"center_crop\"] = transforms.CenterCrop(crop_size)\n",
        "        self.transforms_oprs[\"rdresizecrop\"] = transforms.RandomResizedCrop(crop_size, scale=(0.7, 1.0), ratio=(1,1), interpolation=2)\n",
        "        self.transforms_fun = transforms.Compose([self.transforms_oprs[name] for name in config])\n",
        "\n",
        "    def loader(self, **args):\n",
        "        return DataLoader(dataset=self, **args)\n",
        "\n",
        "    @staticmethod\n",
        "    def read_img(path):\n",
        "        \"\"\"\n",
        "        Read Images\n",
        "        \"\"\"\n",
        "        img = Image.open(path)\n",
        "\n",
        "        return img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV948U-Gu8Ag"
      },
      "source": [
        "ALLMASKTYPES = ['bbox', 'seg', 'random_bbox', 'random_free_form', 'val']\n",
        "input_image = '/content/drive/MyDrive/GatedConvolution_pytorch/result_logs/epoch_60_ckpt.pth.ta/val_0_whole_test_show/real/256x256/39.png'\n",
        "class InpaintDataset(BaseDataset):\n",
        "    \"\"\"\n",
        "    Dataset for Inpainting task\n",
        "    Params:\n",
        "        img_flist_path(str): The file which contains img file path list (e.g. test.flist)\n",
        "        mask_flist_paths_dict(dict): The dict contain the files which contains the pkl or xml file path for\n",
        "                                generate mask. And the key represent the mask type (e.g. {\"bbox\":\"bbox_flist.txt\", \"seg\":..., \"random\":None})\n",
        "        resize_shape(tuple): The shape of the final image (default:(256,256))\n",
        "        transforms_oprs(list) : Determine which transformation used on the imgae (default:['random_crop', 'to_tensor'])\n",
        "        random_bbox_shape(tuple): if use random bbox mask, it define the shape of the mask (default:(32,32))\n",
        "        random_bbox_margin(tuple): if use random bbox, it define the margin of the bbox which means the distance between the mask and the margin of the image\n",
        "                                    (default:(64,64))\n",
        "    Return:\n",
        "        img, *mask\n",
        "    \"\"\"\n",
        "    def __init__(self, img_flist_path, mask_flist_paths_dict,\n",
        "                resize_shape=(256, 256), transforms_oprs=['random_crop', 'to_tensor'],\n",
        "                random_bbox_shape=(32, 32), random_bbox_margin=(64, 64),\n",
        "                random_ff_setting={'img_shape':[256,256],'mv':5, 'ma':4.0, 'ml':40, 'mbw':10}, random_bbox_number=5):\n",
        "\n",
        "        # with open(img_flist_path, 'r') as f:\n",
        "        self.img_paths = [input_image]\n",
        "\n",
        "        self.mask_paths = {}\n",
        "        # for mask_type in mask_flist_paths_dict:\n",
        "        #     #print(mask_type)\n",
        "        #     assert mask_type in ALLMASKTYPES\n",
        "        #     if 'random' in mask_type:\n",
        "        #         self.mask_paths[mask_type] = ['' for i in self.img_paths]\n",
        "        #     else:\n",
        "        #         with open(mask_flist_paths_dict[mask_type]) as f:\n",
        "        #             self.mask_paths[mask_type] = f.read().splitlines()\n",
        "        self.mask_paths['val'] = ['/content/drive/My Drive/GatedConvolution_pytorch/InpaintBenchmark/MaskData/val_ff_100/20.pkl']\n",
        "        # self.mask_paths['val'] = ['/content/drive/MyDrive/GatedConvolution_pytorch/mask.jpeg']\n",
        "\n",
        "        self.resize_shape = resize_shape\n",
        "        self.random_bbox_shape = random_bbox_shape\n",
        "        self.random_bbox_margin = random_bbox_margin\n",
        "        self.random_ff_setting = random_ff_setting\n",
        "        self.random_bbox_number = random_bbox_number\n",
        "        self.transform_initialize(resize_shape, transforms_oprs)\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # create the paths for images and masks\n",
        "\n",
        "        img_path = self.img_paths[index]\n",
        "        error = 1\n",
        "\n",
        "        img = self.transforms_fun(self.read_img(img_path))\n",
        "\n",
        "        mask_paths = {}\n",
        "        for mask_type in self.mask_paths:\n",
        "            mask_paths[mask_type] = self.mask_paths[mask_type][index]\n",
        "\n",
        "        print(type(self.read_img(img_path)))\n",
        "        img = self.transforms_fun(self.read_img(img_path))\n",
        "\n",
        "        masks = {mask_type:255*self.transforms_fun(self.read_mask(mask_paths[mask_type], mask_type))[:1, :,:] for mask_type in mask_paths}\n",
        "        # masks = {'val': self.transforms_fun(self.read_mask(mask_paths['val'], 'val'))}\n",
        "\n",
        "        return img * 255, masks\n",
        "\n",
        "    def read_img(self, path):\n",
        "        \"\"\"\n",
        "        Read Image\n",
        "        \"\"\"\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        return img\n",
        "\n",
        "\n",
        "    def read_mask(self, path, mask_type):\n",
        "        \"\"\"\n",
        "        Read Masks now only support bbox\n",
        "        \"\"\"\n",
        "        if mask_type == 'random_bbox':\n",
        "            bboxs = []\n",
        "            for i in range(self.random_bbox_number):\n",
        "                bbox = InpaintDataset.random_bbox(self.resize_shape, self.random_bbox_margin, self.random_bbox_shape)\n",
        "                bboxs.append(bbox)\n",
        "        elif mask_type == 'random_free_form':\n",
        "            mask = InpaintDataset.random_ff_mask(self.random_ff_setting)\n",
        "            return Image.fromarray(np.tile(mask,(1,1,3)).astype(np.uint8))\n",
        "        elif 'val' in mask_type:\n",
        "            mask = InpaintDataset.read_val_mask(path)\n",
        "            # return mask\n",
        "            return Image.fromarray(np.tile(mask, (1,1,3)).astype(np.uint8))\n",
        "        else:\n",
        "            bbox = InpaintDataset.read_bbox(path)\n",
        "            bboxs = [bbox]\n",
        "        #print(bboxs, self.resize_shape)\n",
        "        mask = InpaintDataset.bbox2mask(bboxs, self.resize_shape)\n",
        "        #print('final', mask.shape)\n",
        "        return Image.fromarray(np.tile(mask,(1,1,3)).astype(np.uint8))\n",
        "\n",
        "    @staticmethod\n",
        "    def read_val_mask(path):\n",
        "        \"\"\"\n",
        "        Read masks from val mask data\n",
        "        \"\"\"\n",
        "        mask = pkl.load(open(path, 'rb'))\n",
        "        # mask = Image.open(path)\n",
        "        # rgb = np.asarray(mask)\n",
        "        # res = (0.2126*rgb[:,:,0]+0.7152*rgb[:,:,1]+0.0722*rgb[:,:,2]).astype(np.uint8)\n",
        "        # img = transforms.ToPILImage()(res)\n",
        "        return mask\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def read_bbox(path):\n",
        "        \"\"\"\n",
        "        The general method for read bbox file by juding the file type\n",
        "        Return:\n",
        "            bbox:[y, x, height, width], shape: (height, width)\n",
        "        \"\"\"\n",
        "        if filename[-3:] == 'pkl' and 'Human' in filename:\n",
        "            return InpaintDataset.read_bbox_ch(filename)\n",
        "        elif filename[-3:] == 'pkl' and 'COCO' in filename:\n",
        "            return InpaintDataset.read_bbox_pkl(filename)\n",
        "        else:\n",
        "            return InpaintDataset.read_bbox_xml(path)\n",
        "\n",
        "    @staticmethod\n",
        "    def read_bbox_xml(path):\n",
        "        \"\"\"\n",
        "        Read bbox for voc xml\n",
        "        Return:\n",
        "            bbox:[y,x,height, width], shape: (height, width)\n",
        "        \"\"\"\n",
        "        with open(filename, 'r') as reader:\n",
        "            xml = reader.read()\n",
        "        soup = BeautifulSoup(xml, 'xml')\n",
        "        size = {}\n",
        "        for tag in soup.size:\n",
        "            if tag.string != \"\\n\":\n",
        "                size[tag.name] = int(tag.string)\n",
        "        objects = soup.find_all('object')\n",
        "        bndboxs = []\n",
        "        for obj in objects:\n",
        "            bndbox = {}\n",
        "            for tag in obj.bndbox:\n",
        "                if tag.string != '\\n':\n",
        "                    bndbox[tag.name] = int(tag.string)\n",
        "\n",
        "            bbox = [bndbox['ymin'], bndbox['xmin'], bndbox['ymax']-bndbox['ymin'], bndbox['xmax']-bndbox['xmin']]\n",
        "            bndboxs.append(bbox)\n",
        "        #print(bndboxs, size)\n",
        "        return bndboxs, (size['height'], size['width'])\n",
        "\n",
        "    @staticmethod\n",
        "    def read_bbox_pkl(path):\n",
        "        \"\"\"\n",
        "        Read bbox from coco pkl\n",
        "        Return:\n",
        "            bbox:[y,x,height, width], shape: (height, width)\n",
        "        \"\"\"\n",
        "        aux_dict = pkl.load(open(path, 'rb'))\n",
        "        bbox = aux_dict[\"bbox\"]\n",
        "        shape = aux_dict[\"shape\"]\n",
        "        #bbox = random.choice(bbox)\n",
        "        #fbox = bbox['fbox']\n",
        "        return [[int(bbox[1]), int(bbox[0]), int(bbox[3]), int(bbox[2])]], (shape[1], shape[0])\n",
        "\n",
        "    @staticmethod\n",
        "    def read_bbox_ch(path):\n",
        "        \"\"\"\n",
        "        Read bbox from crowd human pkl\n",
        "        Return:\n",
        "            bbox:[y,x,height, width], shape: (height, width)\n",
        "        \"\"\"\n",
        "        aux_dict = pkl.load(open(path, 'rb'))\n",
        "        bboxs = aux_dict[\"bbox\"]\n",
        "        bbox = random.choice(bboxs)\n",
        "        extra = bbox['extra']\n",
        "        shape = aux_dict[\"shape\"]\n",
        "        while 'ignore' in extra and extra['ignore'] == 1 and bbox['fbox'][0] < 0 and bbox['fbox'][1] < 0:\n",
        "            bbox = random.choice(bboxs)\n",
        "            extra = bbox['extra']\n",
        "        fbox = bbox['fbox']\n",
        "        return [[fbox[1],fbox[0],fbox[3],fbox[2]]], (shape[1], shape[0])\n",
        "\n",
        "    @staticmethod\n",
        "    def read_seg_img(path):\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def random_bbox(shape, margin, bbox_shape):\n",
        "        \"\"\"Generate a random tlhw with configuration.\n",
        "        Args:\n",
        "            config: Config should have configuration including IMG_SHAPES,\n",
        "                VERTICAL_MARGIN, HEIGHT, HORIZONTAL_MARGIN, WIDTH.\n",
        "        Returns:\n",
        "            tuple: (top, left, height, width)\n",
        "        \"\"\"\n",
        "        img_height = shape[0]\n",
        "        img_width = shape[1]\n",
        "        height, width = bbox_shape\n",
        "        ver_margin, hor_margin = margin\n",
        "        maxt = img_height - ver_margin - height\n",
        "        maxl = img_width - hor_margin - width\n",
        "        t = np.random.randint(low=ver_margin, high=maxt)\n",
        "        l = np.random.randint(low=hor_margin, high=maxl)\n",
        "        h = height\n",
        "        w = width\n",
        "        return (t, l, h, w)\n",
        "\n",
        "    @staticmethod\n",
        "    def random_ff_mask(config):\n",
        "        \"\"\"Generate a random free form mask with configuration.\n",
        "        Args:\n",
        "            config: Config should have configuration including IMG_SHAPES,\n",
        "                VERTICAL_MARGIN, HEIGHT, HORIZONTAL_MARGIN, WIDTH.\n",
        "        Returns:\n",
        "            tuple: (top, left, height, width)\n",
        "        \"\"\"\n",
        "\n",
        "        h,w = config['img_shape']\n",
        "        mask = np.zeros((h,w))\n",
        "        num_v = 12+np.random.randint(config['mv'])#tf.random_uniform([], minval=0, maxval=config.MAXVERTEX, dtype=tf.int32)\n",
        "\n",
        "        for i in range(num_v):\n",
        "            start_x = np.random.randint(w)\n",
        "            start_y = np.random.randint(h)\n",
        "            for j in range(1+np.random.randint(5)):\n",
        "                angle = 0.01+np.random.randint(config['ma'])\n",
        "                if i % 2 == 0:\n",
        "                    angle = 2 * 3.1415926 - angle\n",
        "                length = 10+np.random.randint(config['ml'])\n",
        "                brush_w = 10+np.random.randint(config['mbw'])\n",
        "                end_x = (start_x + length * np.sin(angle)).astype(np.int32)\n",
        "                end_y = (start_y + length * np.cos(angle)).astype(np.int32)\n",
        "\n",
        "                cv2.line(mask, (start_y, start_x), (end_y, end_x), 1.0, brush_w)\n",
        "                start_x, start_y = end_x, end_y\n",
        "\n",
        "        return mask.reshape(mask.shape+(1,)).astype(np.float32)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def bbox2mask(bboxs, shape):\n",
        "        \"\"\"Generate mask tensor from bbox.\n",
        "        Args:\n",
        "            bbox: configuration tuple, (top, left, height, width)\n",
        "            config: Config should have configuration including IMG_SHAPES,\n",
        "                MAX_DELTA_HEIGHT, MAX_DELTA_WIDTH.\n",
        "        Returns:\n",
        "            tf.Tensor: output with shape [1, H, W, 1]\n",
        "        \"\"\"\n",
        "        height, width = shape\n",
        "        mask = np.zeros(( height, width), np.float32)\n",
        "        #print(mask.shape)\n",
        "        for bbox in bboxs:\n",
        "            h = int(0.1*bbox[2])+np.random.randint(int(bbox[2]*0.2+1))\n",
        "            w = int(0.1*bbox[3])+np.random.randint(int(bbox[3]*0.2)+1)\n",
        "            mask[bbox[0]+h:bbox[0]+bbox[2]-h,\n",
        "                 bbox[1]+w:bbox[1]+bbox[3]-w] = 1.\n",
        "        #print(\"after\", mask.shape)\n",
        "        return mask.reshape(mask.shape+(1,)).astype(np.float32)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpBsK1xfxoFo",
        "outputId": "e01422d5-c0bc-4c4a-8ac5-47442357adf3"
      },
      "source": [
        "val = InpaintDataset('/content/drive/My Drive/GatedConvolution_pytorch/SingleImage/SingleImage.flist',\\\n",
        "                                    {'val': '/content/drive/MyDrive/GatedConvolution_pytorch/SingleImage/SingleMask.flist',}, \\\n",
        "                                )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:835: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9MLJi0coUkZ"
      },
      "source": [
        "## TESTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SN-kI4iBujl6",
        "outputId": "e7268690-4f6b-4200-936f-7677a32652e5"
      },
      "source": [
        "cd /content/drive/MyDrive/GatedConvolution_pytorch/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1eIjWo9Go3uUeNclo762g1za05CKewpV3/GatedConvolution_pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0cfzWgU6tDv"
      },
      "source": [
        "def load_consistent_state_dict(pretrained_dict, model):\n",
        "    model_dict = model.state_dict()\n",
        "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and 'coarse' in k}\n",
        "    # 2. overwrite entries in the existing state dict\n",
        "    model_dict.update(pretrained_dict)\n",
        "    # 3. load the new state dict\n",
        "    model.load_state_dict(model_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZHr7ZrJoXd5"
      },
      "source": [
        "whole_model_path = 'model_logs/epoch_60_ckpt.pth.tar'\n",
        "nets = torch.load(whole_model_path)\n",
        "netG_state_dict, netD_state_dict = nets['netG_state_dict'], nets['netD_state_dict']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iwlJ_YrfLbG",
        "outputId": "b514b916-2810-450b-b567-e44de92de63b"
      },
      "source": [
        "imgs = val[0][0]\n",
        "masks = val[0][1]['val']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'PIL.Image.Image'>\n",
            "<class 'PIL.Image.Image'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19NncWnBFLK8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "outputId": "bd4c131b-ac09-4c09-9a35-12f05b30ac50"
      },
      "source": [
        "msk = transforms.ToPILImage()(masks)\n",
        "msk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=256x256 at 0x7F0824F3E510>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgwkNtC7m_vs"
      },
      "source": [
        "class InpaintSANet(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Inpaint generator, input should be 5*256*256, where 3*256*256 is the masked image, 1*256*256 for mask, 1*256*256 is the guidence\n",
        "    \"\"\"\n",
        "    def __init__(self, n_in_channel=5):\n",
        "        super(InpaintSANet, self).__init__()\n",
        "        cnum = 32\n",
        "        self.coarse_net = nn.Sequential(\n",
        "            #input is 5*256*256, but it is full convolution network, so it can be larger than 256\n",
        "            GatedConv2dWithActivation(n_in_channel, cnum, 5, 1, padding=get_pad(256, 5, 1)),\n",
        "            # downsample 128\n",
        "            GatedConv2dWithActivation(cnum, 2*cnum, 4, 2, padding=get_pad(256, 4, 2)),\n",
        "            GatedConv2dWithActivation(2*cnum, 2*cnum, 3, 1, padding=get_pad(128, 3, 1)),\n",
        "            #downsample to 64\n",
        "            GatedConv2dWithActivation(2*cnum, 4*cnum, 4, 2, padding=get_pad(128, 4, 2)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "            # atrous convlution\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=2, padding=get_pad(64, 3, 1, 2)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=4, padding=get_pad(64, 3, 1, 4)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=8, padding=get_pad(64, 3, 1, 8)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=16, padding=get_pad(64, 3, 1, 16)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "            #Self_Attn(4*cnum, 'relu'),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "            # upsample\n",
        "            GatedDeConv2dWithActivation(2, 4*cnum, 2*cnum, 3, 1, padding=get_pad(128, 3, 1)),\n",
        "            #Self_Attn(2*cnum, 'relu'),\n",
        "            GatedConv2dWithActivation(2*cnum, 2*cnum, 3, 1, padding=get_pad(128, 3, 1)),\n",
        "            GatedDeConv2dWithActivation(2, 2*cnum, cnum, 3, 1, padding=get_pad(256, 3, 1)),\n",
        "\n",
        "            GatedConv2dWithActivation(cnum, cnum//2, 3, 1, padding=get_pad(256, 3, 1)),\n",
        "            #Self_Attn(cnum//2, 'relu'),\n",
        "            GatedConv2dWithActivation(cnum//2, 3, 3, 1, padding=get_pad(128, 3, 1), activation=None)\n",
        "        )\n",
        "\n",
        "        self.refine_conv_net = nn.Sequential(\n",
        "            # input is 5*256*256\n",
        "            GatedConv2dWithActivation(n_in_channel, cnum, 5, 1, padding=get_pad(256, 5, 1)),\n",
        "            # downsample\n",
        "            GatedConv2dWithActivation(cnum, cnum, 4, 2, padding=get_pad(256, 4, 2)),\n",
        "            GatedConv2dWithActivation(cnum, 2*cnum, 3, 1, padding=get_pad(128, 3, 1)),\n",
        "            # downsample\n",
        "            GatedConv2dWithActivation(2*cnum, 2*cnum, 4, 2, padding=get_pad(128, 4, 2)),\n",
        "            GatedConv2dWithActivation(2*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=2, padding=get_pad(64, 3, 1, 2)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=4, padding=get_pad(64, 3, 1, 4)),\n",
        "            # Self_Attn(4*cnum, 'relu'),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=8, padding=get_pad(64, 3, 1, 8)),\n",
        "\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=16, padding=get_pad(64, 3, 1, 16))\n",
        "        )\n",
        "        self.refine_attn = Self_Attn(4*cnum, 'relu', with_attn=False)\n",
        "        self.refine_upsample_net = nn.Sequential(\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "            GatedDeConv2dWithActivation(2, 4*cnum, 2*cnum, 3, 1, padding=get_pad(128, 3, 1)),\n",
        "            GatedConv2dWithActivation(2*cnum, 2*cnum, 3, 1, padding=get_pad(128, 3, 1)),\n",
        "            GatedDeConv2dWithActivation(2, 2*cnum, cnum, 3, 1, padding=get_pad(256, 3, 1)),\n",
        "\n",
        "            GatedConv2dWithActivation(cnum, cnum//2, 3, 1, padding=get_pad(256, 3, 1)),\n",
        "            # Self_Attn(cnum, 'relu'),\n",
        "            GatedConv2dWithActivation(cnum//2, 3, 3, 1, padding=get_pad(256, 3, 1), activation=None),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, imgs, masks, img_exs=None):\n",
        "        # Coarse\n",
        "        masked_imgs =  imgs * (1 - masks) + masks\n",
        "        if img_exs == None:\n",
        "            input_imgs = torch.cat([masked_imgs, masks, torch.full_like(masks, 1.)])\n",
        "        else:\n",
        "            input_imgs = torch.cat([masked_imgs, img_exs, masks, torch.full_like(masks, 1.)], dim=1)\n",
        "        input_imgs = torch.reshape(input_imgs, [1, 5, 256, 256])\n",
        "        x = self.coarse_net(input_imgs)\n",
        "        x = torch.clamp(x, -1., 1.)\n",
        "        coarse_x = x\n",
        "        # Refine\n",
        "\n",
        "        masked_imgs = imgs * (1 - masks) + coarse_x * masks\n",
        "        masked_imgs = masked_imgs[0]\n",
        "        if img_exs is None:\n",
        "            input_imgs = torch.cat([masked_imgs, masks, torch.full_like(masks, 1.)])\n",
        "        else:\n",
        "            input_imgs = torch.cat([masked_imgs, img_exs, masks, torch.full_like(masks, 1.)], dim=1)\n",
        "        input_imgs = torch.reshape(input_imgs, [1, 5, 256, 256])\n",
        "        x = self.refine_conv_net(input_imgs)\n",
        "        x = self.refine_attn(x)\n",
        "        # print(x.size())\n",
        "        x = self.refine_upsample_net(x)\n",
        "        x = torch.clamp(x, -1., 1.)\n",
        "        return coarse_x, x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_4F_O9TphM5"
      },
      "source": [
        "netG = InpaintSANet()\n",
        "netD = InpaintSADirciminator()\n",
        "\n",
        "# load_consistent_state_dict(netG_state_dict, netG)\n",
        "netG.load_state_dict(netG_state_dict)\n",
        "\n",
        "netG.eval()\n",
        "netD.eval()\n",
        "\n",
        "netG.train()\n",
        "netD.train()\n",
        "\n",
        "lr, decay = 0.0001, 0.0\n",
        "optG = torch.optim.Adam(netG.parameters(), lr=lr, weight_decay=decay)\n",
        "optD = torch.optim.Adam(netD.parameters(), lr=4*lr, weight_decay=decay)\n",
        "\n",
        "def img2photo(imgs):\n",
        "    return ((imgs + 1) * 127.5).transpose(1, 2).transpose(2, 3).detach().cpu().numpy()\n",
        "\n",
        "mask = F.interpolate(masks, 256)\n",
        "mask = (masks > 0).type(torch.FloatTensor)\n",
        "img = F.interpolate(imgs, 256)\n",
        "\n",
        "\n",
        "input = (img / 127.5 - 1)\n",
        "input_img = img2photo(input.unsqueeze(0))\n",
        "recon_imgs, refine = netG(input, mask)\n",
        "complete_imgs = refine * mask + input * (1 - mask)\n",
        "\n",
        "\n",
        "\n",
        "gen_img = img2photo(recon_imgs)\n",
        "refine_img = img2photo(refine)\n",
        "comp_img = img2photo(complete_imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DDHRXL-Ujl4",
        "outputId": "007554a6-7dbc-4b86-a06e-e7c2f3584ba0"
      },
      "source": [
        "!pip install torchviz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.9.0+cu102)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (3.7.4.3)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4151 sha256=5e4ce20ab734ff1f3dce1086ea348dd89caa897e6703264f7ec504b3a3d19aac\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/38/f5/dc4f85c3909051823df49901e72015d2d750bd26b086480ec2\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liXQg30iRY--",
        "outputId": "68dfe12f-a523-4aeb-9525-74d9513eda06"
      },
      "source": [
        "netG"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "InpaintSANet(\n",
              "  (coarse_net): Sequential(\n",
              "    (0): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(5, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "      (mask_conv2d): Conv2d(5, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "      (batch_norm2d): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (1): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (mask_conv2d): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (batch_norm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (2): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (mask_conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch_norm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (3): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (mask_conv2d): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (batch_norm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (4): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (mask_conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch_norm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (5): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (mask_conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch_norm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (6): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
              "      (mask_conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
              "      (batch_norm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (7): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))\n",
              "      (mask_conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))\n",
              "      (batch_norm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (8): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8))\n",
              "      (mask_conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8))\n",
              "      (batch_norm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (9): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16))\n",
              "      (mask_conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16))\n",
              "      (batch_norm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (10): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (mask_conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch_norm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (11): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (mask_conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch_norm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (12): GatedDeConv2dWithActivation(\n",
              "      (conv2d): GatedConv2dWithActivation(\n",
              "        (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "        (conv2d): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (mask_conv2d): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch_norm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (sigmoid): Sigmoid()\n",
              "      )\n",
              "    )\n",
              "    (13): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (mask_conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch_norm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (14): GatedDeConv2dWithActivation(\n",
              "      (conv2d): GatedConv2dWithActivation(\n",
              "        (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "        (conv2d): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (mask_conv2d): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch_norm2d): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (sigmoid): Sigmoid()\n",
              "      )\n",
              "    )\n",
              "    (15): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (mask_conv2d): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch_norm2d): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (16): GatedConv2dWithActivation(\n",
              "      (conv2d): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (mask_conv2d): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch_norm2d): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "  )\n",
              "  (refine_conv_net): Sequential(\n",
              "    (0): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(5, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "      (mask_conv2d): Conv2d(5, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "      (batch_norm2d): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (1): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (mask_conv2d): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (batch_norm2d): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (2): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (mask_conv2d): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch_norm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (3): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (mask_conv2d): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (batch_norm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (4): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (mask_conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch_norm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (5): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (mask_conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch_norm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (6): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (mask_conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch_norm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (7): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
              "      (mask_conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
              "      (batch_norm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (8): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))\n",
              "      (mask_conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))\n",
              "      (batch_norm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (9): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8))\n",
              "      (mask_conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8))\n",
              "      (batch_norm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (10): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16))\n",
              "      (mask_conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16))\n",
              "      (batch_norm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "  )\n",
              "  (refine_attn): Self_Attn(\n",
              "    (query_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (key_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (value_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (softmax): Softmax(dim=-1)\n",
              "  )\n",
              "  (refine_upsample_net): Sequential(\n",
              "    (0): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (mask_conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch_norm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (1): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (mask_conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch_norm2d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (2): GatedDeConv2dWithActivation(\n",
              "      (conv2d): GatedConv2dWithActivation(\n",
              "        (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "        (conv2d): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (mask_conv2d): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch_norm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (sigmoid): Sigmoid()\n",
              "      )\n",
              "    )\n",
              "    (3): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (mask_conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch_norm2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (4): GatedDeConv2dWithActivation(\n",
              "      (conv2d): GatedConv2dWithActivation(\n",
              "        (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "        (conv2d): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (mask_conv2d): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (batch_norm2d): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (sigmoid): Sigmoid()\n",
              "      )\n",
              "    )\n",
              "    (5): GatedConv2dWithActivation(\n",
              "      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "      (conv2d): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (mask_conv2d): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch_norm2d): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (6): GatedConv2dWithActivation(\n",
              "      (conv2d): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (mask_conv2d): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch_norm2d): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt41vHz49X9i"
      },
      "source": [
        "overlap = input * (1-mask)\n",
        "print(overlap.shape)\n",
        "overlap.unsqueeze(0)\n",
        "\n",
        "I = img2photo(overlap.unsqueeze(0))\n",
        "Image.fromarray(I[0].astype(np.uint8))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afSeU4K69UhX"
      },
      "source": [
        "Image.fromarray(input_img[0].astype(np.uint8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOOlK8odszIb"
      },
      "source": [
        "Image.open(input_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gd5Unp1vJ5eD"
      },
      "source": [
        "genr_img = Image.fromarray(gen_img[0].astype(np.uint8))\n",
        "genr_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXbk18IzzimP"
      },
      "source": [
        "ref_img = Image.fromarray(refine_img[0].astype(np.uint8))\n",
        "ref_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2tkDpbiKS7i"
      },
      "source": [
        "compl_img = Image.fromarray(comp_img[0].astype(np.uint8))\n",
        "compl_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLiBxkBiWDsN"
      },
      "source": [
        "import numpy\n",
        "from numpy import cov\n",
        "from numpy import trace\n",
        "from numpy import iscomplexobj\n",
        "from numpy import asarray\n",
        "from numpy.random import randint\n",
        "from scipy.linalg import sqrtm\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.datasets.mnist import load_data\n",
        "from skimage.transform import resize\n",
        " \n",
        "# scale an array of images to a new size\n",
        "def scale_images(images, new_shape):\n",
        "\timages_list = list()\n",
        "\tfor image in images:\n",
        "\t\t# resize with nearest neighbor interpolation\n",
        "\t\tnew_image = resize(image, new_shape, 0)\n",
        "\t\t# store\n",
        "\t\timages_list.append(new_image)\n",
        "\treturn asarray(images_list)\n",
        " \n",
        "# calculate frechet inception distance\n",
        "def calculate_fid(model, images1, images2):\n",
        "\t# calculate activations\n",
        "\tact1 = model.predict(images1)\n",
        "\tact2 = model.predict(images2)\n",
        "\t# calculate mean and covariance statistics\n",
        "\tmu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)\n",
        "\tmu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)\n",
        "\t# calculate sum squared difference between means\n",
        "\tssdiff = numpy.sum((mu1 - mu2)**2.0)\n",
        "\t# calculate sqrt of product between cov\n",
        "\tcovmean = sqrtm(sigma1.dot(sigma2))\n",
        "\t# check and correct imaginary numbers from sqrt\n",
        "\tif iscomplexobj(covmean):\n",
        "\t\tcovmean = covmean.real\n",
        "\t# calculate score\n",
        "\tfid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "\treturn fid\n",
        " \n",
        "# prepare the inception v3 model\n",
        "model = InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3))\n",
        "# define two fake collections of images\n",
        "# images1 = randint(0, 255, 10*32*32*3)\n",
        "# images1 = images1.reshape((10,32,32,3))\n",
        "# images2 = randint(0, 255, 10*32*32*3)\n",
        "# images2 = images2.reshape((10,32,32,3))\n",
        "images1 = input_img[0]\n",
        "images2 = gen_img[0]\n",
        "print('Prepared', images1.shape, images2.shape)\n",
        "# convert integer to floating point values\n",
        "images1 = images1.astype('float32')\n",
        "images2 = images2.astype('float32')\n",
        "# resize images\n",
        "images1 = scale_images(images1, (299,299,3))\n",
        "images2 = scale_images(images2, (299,299,3))\n",
        "print('Scaled', images1.shape, images2.shape)\n",
        "# pre-process images\n",
        "images1 = preprocess_input(images1)\n",
        "images2 = preprocess_input(images2)\n",
        "# fid between images1 and images1\n",
        "fid = calculate_fid(model, images1, images1)\n",
        "print('FID (same): %.3f' % fid)\n",
        "# fid between images1 and images2\n",
        "fid = calculate_fid(model, images1, images2)\n",
        "print('FID (different): %.3f' % fid)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}